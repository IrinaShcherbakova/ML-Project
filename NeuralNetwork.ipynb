{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "\n",
    "# WordCloud and matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "\n",
    "- assigning keywords to missing keyword labels - DONE\n",
    "- remove symbols (#, \".. - DONE\n",
    "- remove links - DONE\n",
    "- validation\n",
    "- make slides - IN PROCESS\n",
    "- TFIDF, LSA, LSTM / RNNs, the list is long\n",
    "- try different classifiers\n",
    "- reading discussion board for inspiration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"preprocessed_train_data.csv\", index_col = 0)\n",
    "test = pd.read_csv(\"preprocessed_test_data.csv\", index_col = 0)\n",
    "\n",
    "trained_tweets = train['keyword']+train['text']\n",
    "test_tweets = test['keyword']+test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    earthquak  deed reason earthquak may allah for...\n",
       "1     forest fire  forest fire near la rang ask canada\n",
       "2    evacu  resid ask shelter place notifi offic ev...\n",
       "3    wildfir  peopl receiv wildfir evacu order cali...\n",
       "4    wildfir  got sent photo rubi alaska smoke wild...\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      crash  happen terribl car crash\n",
       "1    earthquak  heard earthquak differ citi stay sa...\n",
       "2    forest fire  forest fire spot pond gees flee a...\n",
       "3            apocalyps  apocalyps light spokan wildfir\n",
       "4          typhoon  typhoon soudelor kill china taiwan\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding and Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import category_encoders as ce\n",
    "\n",
    "# # Target encoding\n",
    "# features = ['keyword']\n",
    "# encoder = ce.TargetEncoder(cols=features)\n",
    "# encoder.fit(train[features],train['target'])\n",
    "\n",
    "# train = train.join(encoder.transform(train[features]).add_suffix('_target'))\n",
    "# test = test.join(encoder.transform(test[features]).add_suffix('_target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(trained_tweets, train['target'].values, \n",
    "        train_size = 0.80, test_size = 0.20, random_state = 12, shuffle=True) # We have split train-test dataset using 80:20 ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6090, 48864) (1523, 48864)\n"
     ]
    }
   ],
   "source": [
    "#precompute vectorized representations\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 3),\n",
    "    lowercase=True,\n",
    "    min_df=5,\n",
    "    max_features=30000)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(3, 6),\n",
    "    lowercase=True,\n",
    "    min_df=5,\n",
    "    max_features=50000)\n",
    "\n",
    "vectorizer = FeatureUnion([('word_vectorizer', word_vectorizer),  ('char_vectorizer', char_vectorizer)])\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_train_vectors = vectorizer.transform(X_train).toarray()\n",
    "X_test_vectors = vectorizer.transform(X_test).toarray()\n",
    "print(X_train_vectors.shape, X_test_vectors.shape)\n",
    "\n",
    "#X_train_text = X_train.tolist()\n",
    "#X_test_text = X_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "## Tokenize the sentences\n",
    "max_features=50000\n",
    "maxlen=25\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train.tolist())\n",
    "X_train = tokenizer.texts_to_sequences(X_train.tolist())\n",
    "X_test = tokenizer.texts_to_sequences(X_test.tolist())\n",
    "\n",
    "## Pad the sentences \n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  353,  716,  838],\n",
       "       [   0,    0,    0, ...,   39,  283,  403],\n",
       "       [   0,    0,    0, ..., 1614,    9, 2313],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 1496,  947,  310],\n",
       "       [   0,    0,    0, ...,  364,  513, 1019],\n",
       "       [   0,    0,    0, ...,  213,   73,  729]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10401 101\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, GlobalAveragePooling1D, GlobalMaxPooling1D, Dropout, Embedding\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "vocab_dim = X_train.max() + 1\n",
    "embed_dim = int(np.sqrt(vocab_dim))\n",
    "print(vocab_dim, embed_dim)\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_dim, output_dim=embed_dim),\n",
    "    SimpleRNN(units=embed_dim, return_sequences=False),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/5\n",
      "6090/6090 [==============================] - 5s 821us/sample - loss: 0.5847 - accuracy: 0.6849 - val_loss: 0.4948 - val_accuracy: 0.7833\n",
      "Epoch 2/5\n",
      "6090/6090 [==============================] - 3s 512us/sample - loss: 0.2561 - accuracy: 0.9046 - val_loss: 0.5635 - val_accuracy: 0.7577\n",
      "Epoch 3/5\n",
      "6090/6090 [==============================] - 3s 543us/sample - loss: 0.1218 - accuracy: 0.9624 - val_loss: 0.5828 - val_accuracy: 0.7656\n",
      "Epoch 4/5\n",
      "6090/6090 [==============================] - 3s 521us/sample - loss: 0.0820 - accuracy: 0.9739 - val_loss: 0.6205 - val_accuracy: 0.7669\n",
      "Epoch 5/5\n",
      "6090/6090 [==============================] - 4s 583us/sample - loss: 0.0653 - accuracy: 0.9793 - val_loss: 0.7195 - val_accuracy: 0.7702\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=X_train, y=y_train, batch_size=64, epochs=5, verbose=True, validation_data=(X_test, y_test), shuffle=True\n",
    ")\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_dim, output_dim=embed_dim),\n",
    "    SimpleRNN(units=embed_dim, return_sequences=True),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/5\n",
      "6090/6090 [==============================] - 5s 843us/sample - loss: 0.6048 - accuracy: 0.6813 - val_loss: 0.4801 - val_accuracy: 0.7794\n",
      "Epoch 2/5\n",
      "6090/6090 [==============================] - 3s 540us/sample - loss: 0.3515 - accuracy: 0.8576 - val_loss: 0.5017 - val_accuracy: 0.7676\n",
      "Epoch 3/5\n",
      "6090/6090 [==============================] - 4s 606us/sample - loss: 0.2208 - accuracy: 0.9228 - val_loss: 0.5292 - val_accuracy: 0.7702\n",
      "Epoch 4/5\n",
      "6090/6090 [==============================] - 4s 606us/sample - loss: 0.1462 - accuracy: 0.9529 - val_loss: 0.6095 - val_accuracy: 0.7669\n",
      "Epoch 5/5\n",
      "6090/6090 [==============================] - 4s 640us/sample - loss: 0.1032 - accuracy: 0.9693 - val_loss: 0.7132 - val_accuracy: 0.7682\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=X_train, y=y_train, batch_size=64, epochs=5, verbose=True, validation_data=(X_test, y_test), shuffle=True\n",
    ")\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_dim, output_dim=embed_dim),\n",
    "    Bidirectional(SimpleRNN(units=embed_dim, return_sequences=True)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/5\n",
      "6090/6090 [==============================] - 9s 2ms/sample - loss: 0.5807 - accuracy: 0.7025 - val_loss: 0.4848 - val_accuracy: 0.7958\n",
      "Epoch 2/5\n",
      "6090/6090 [==============================] - 6s 962us/sample - loss: 0.3223 - accuracy: 0.8696 - val_loss: 0.4991 - val_accuracy: 0.7866\n",
      "Epoch 3/5\n",
      "6090/6090 [==============================] - 5s 896us/sample - loss: 0.1884 - accuracy: 0.9315 - val_loss: 0.5673 - val_accuracy: 0.7800\n",
      "Epoch 4/5\n",
      "6090/6090 [==============================] - 6s 928us/sample - loss: 0.1126 - accuracy: 0.9614 - val_loss: 0.6587 - val_accuracy: 0.7400\n",
      "Epoch 5/5\n",
      "6090/6090 [==============================] - 6s 975us/sample - loss: 0.0854 - accuracy: 0.9703 - val_loss: 0.6266 - val_accuracy: 0.7663\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=X_train, y=y_train, batch_size=64, epochs=5, verbose=True, validation_data=(X_test, y_test), shuffle=True\n",
    ")\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_dim, output_dim=embed_dim),\n",
    "    LSTM(units=embed_dim, return_sequences=False),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/5\n",
      "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.5628 - accuracy: 0.7103 - val_loss: 0.4604 - val_accuracy: 0.7925\n",
      "Epoch 2/5\n",
      "6090/6090 [==============================] - 8s 1ms/sample - loss: 0.3171 - accuracy: 0.8742 - val_loss: 0.4866 - val_accuracy: 0.7814\n",
      "Epoch 3/5\n",
      "6090/6090 [==============================] - 8s 1ms/sample - loss: 0.2075 - accuracy: 0.9232 - val_loss: 0.6113 - val_accuracy: 0.7689\n",
      "Epoch 4/5\n",
      "6090/6090 [==============================] - 8s 1ms/sample - loss: 0.1446 - accuracy: 0.9461 - val_loss: 0.6945 - val_accuracy: 0.7728\n",
      "Epoch 5/5\n",
      "6090/6090 [==============================] - 8s 1ms/sample - loss: 0.1063 - accuracy: 0.9637 - val_loss: 0.7461 - val_accuracy: 0.7702\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=X_train, y=y_train, batch_size=64, epochs=5, verbose=True, validation_data=(X_test, y_test), shuffle=True\n",
    ")\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
